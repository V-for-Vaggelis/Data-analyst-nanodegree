{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "This project is about combining and cleaning different data sources to make them easily usable for visualization, models etc. The data are about tweets of dogs, specifically there are three different data sources:\n",
    "* The WeRateDogs Twitter archive, provided as a csv file called **twitter-archive-enhanced.csv**\n",
    "* The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. These data are hosted in the Udacity servers and accessed through an http request. The response is a tsv file called **image_predictions.tsv**\n",
    "* Each tweet's retweet count and favorite (\"like\") count, accessed through tweeter's API and stored programmatically in a txt file called **tweet_json.txt**.\n",
    "\n",
    "### Loading and cleaning the locally stored csv called twitter-archive-enhanced.csv\n",
    "Loading a locally stored file is pretty straightforward. After that and by performing some assesements, the issues below are noted and handled. All the cleaning is performed in a copy of the original dataframe called **arch_clean_df**:<br><br>\n",
    "**tidyness issues:**\n",
    "* The dog-state information occupies 4 columns that actually repeat the column name when it is true. This is handled by replacing the 4 columns with one called \"dog_state\". For tweets with multiple dogs (that also have multiple states) we provide 2 or more values seperated by commas in the corresponding cell\n",
    "* The \"denominator\" column is the same for all rows, it is redundant since we can provide a single column for the ratings that could hold a float as a result. This is handled by replacing the \"rating_numerator\" and \"rating_denominator\" values with the product of their division (but not before we re-capture the numerator so that ignored decimals that were missing in some cases are now in place). Now a single column called \"rating\" exists.\n",
    "<br>\n",
    "\n",
    "**cleaniness issues:**\n",
    "* Several columns ('in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp') inform us whether we are dealing with an original tweet or not. In detail wherever the first two have values we are dealing with replies to tweets and wherever the 3 others do we are dealing with retweets. Since we are interested in original tweets only we have removed all the rows where the 5 columns above have non-null values. After that the 5 columns themselves are deleted since they don't provide any information.\n",
    "* \"text\" is not a very descriptive column name. We changed this to \"dog_description\"\n",
    "* The \"source\" column is the same for all rows and is an html tag rather than a raw link. This could be fixed anytime, but since we didn't use that column we left it as it is though\n",
    "* The \"tweet_id\" column should be of type object (string) and not integer. This is fixed\n",
    "* The rating numerator and denominator should be floats. Also they need to be more carefully extracted from the text so that decimal points are preserved. This is fixed before replacing those with the \"rating\" column\n",
    "* The dog names are often wrong, random words are in place of names. They could be programmatically identified because they don't begin with a capital character. We could fix this but since we won't be using the names we just dropped that column, in any case names can be extracted by the \"dog_description\" at any point\n",
    "\n",
    "In conclusion, **2 tidyness and 5 cleaning issues were handled in the twitter-archive-enhanced.csv data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programatically downloading and cleaning the data file called image_predictions.tsv\n",
    "In order to access this file an http request is necessary since it is stored in Udacity's database. A piece of code that utilizes the os and the requests library is created to download this file only in case it does not already exist in the working directory. After that loading the file is the same process as before. This file is more tidy and clean, some issues are noted and below. All the cleaning is performed in a copy of the original dataframe called pred_clean_df:<br><br>\n",
    "\n",
    "**cleaniness issues:**\n",
    "* Since the algorithm provides 3 guesses, a certainty value for each and boolean to whether that guess corresponds to a dog or not, we use this information to only keep valid dog breeds with a conf. interval larger than 0.5. So after all this is a tidyness issue because the \"dog\" boolean columns aren't even required if proper filtering is applied in the data and \"breeds\" as well as \"conf. intervals\" can be stored in one column each. So the 9 columns originally present in the data can be replaced by just two\n",
    "* We could add more descriptive column names in several occasions. Specifically, \"img_num\" is changed to \"pred_sample_size\" and the best guess columns we provide by the first issue are named \"breed_pred\" and \"breed_conf\"\n",
    "* Again the \"tweet_id\" column should be of object type, we have handled this\n",
    "\n",
    "In conclusion, **3 cleaning issues were handled in the image_predictions.tsv data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering, saving and cleaning data from the twitter api\n",
    "At first, we provide our secret twitter api keys and tokens via a local script that is not visible in the notebook. The an api object is created with some parameters that can handle the request-rate limit. At first a single request is given and we print the result so that we get an idea of the response object. Then a piece of code is created that completes the following operations:\n",
    "1. Checks if the 'tweet_json.txt' file exists in the working directory and terminates if so\n",
    "2. If it does not, it iteratively sends a request for every tweet id in the two previous datasets\n",
    "3. If a tweet is found, it writes the response in a line in the \"tweet_json.txt\" file, if not it saves the \"failed\" request tweet id in an array\n",
    "4. At the end a message informs that the file was created and how many tweets were succesfully found.\n",
    "\n",
    "Reading the 'tweet_json.txt' file is not as straightforward as in the previous cases because it contains a large ammount of information, which is why we target the parameters that interest us ('tweet_id', 'retweet_count', 'favorite_count'). We loop the file line by line and pull those parameters, we store them in a dictionary that is later converted in a dataframe.\n",
    "\n",
    "Since we only pull specific parameters directly and we control the process the data are pretty tidy and clean. Again we fix the \"tweet_id\" being integer instead of object issue.\n",
    "\n",
    "In conclusion, creating and reading this file is the hardest part and there are many cleaning issues. **Just 1 cleaning issue is handled in the 'tweet_json.txt' data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the three data sources, cleaning occuring issues and saving the result\n",
    "Our next goal is to merge the three dataframes, namely: **arch_clean_df, pred_clean_df, tweet_api_df**. All three of them definetly have the \"tweet_id\" column so it can be used as the key, but we need to keep in mind that they probably have different number of rows. If we use the default inner join, all rows that don't match will be lost, so instead we merge the first two with a left join, and their product is merged with the third dataframe with another left join. The final dataframe is called \n",
    "\n",
    "One issue that arises is that some columns are converted to float from integer (\"pred_sample_size\", \"retweet_count\", \"fav_count\"). Pandas automatically does this sometimes for memory reasons, especially if many nan values are present in the columns. This is fixed with some information provided <a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/gotchas.html#support-for-integer-na\">here</a>\n",
    "\n",
    "In conclusion **a tidyness issue is handled by merging the 3 datasets and 1 cleaning issue is handled in the merged dataset**\n",
    "\n",
    "After that, our data are in the form we want it, so we store this final dataframe in a file called **'twitter_archive_master.csv'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
